1. Installation and Setup

- Step 1: Install Apache Spark
  Go to the Apache Spark website (https://spark.apache.org/downloads.html).
  Download the latest stable version of Apache Spark.
  Extract the downloaded file to your desired location.
- Step 2: Install Java Development Kit (JDK)
  PySpark requires Java to run. Install the Java Development Kit (JDK) if you donâ€™t have it installed already.
  Download and install the JDK from the Oracle website (https://www.oracle.com/java/technologies/javase-jdk11-downloads.html).
- Step 3: Set Environment Variables
  Open your terminal or command prompt.
  Set the SPARK_HOME environment variable to the location where you extracted Apache Spark.
  Append the bin directory of Apache Spark to your PATH environment variable.
  Set the JAVA_HOME environment variable to the location where you installed the JDK.
- Step 4: Install PySpark
  Open your terminal or command prompt.
  Run the following command to install PySpark using pip:
pip install pyspark

#---------------------------------------------------------------------------

from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder.appName("PySparkExample").getOrCreate()

# Create a DataFrame
data = [("Alice", 25), ("Bob", 30), ("Charlie", 35)]
df = spark.createDataFrame(data, ["Name", "Age"])

# Print the DataFrame
df.show()

# Perform a simple transformation
df_filtered = df.filter(df["Age"] > 30)

# Print the filtered DataFrame
df_filtered.show()

# Stop the SparkSession
spark.stop()

#----------------------------------------------------------------------

# Selecting specific columns
df.select("Name", "Age").show()

# Adding a new column
df_with_gender = df.withColumn("Gender", "Female")
df_with_gender.show()

# Filtering rows based on a condition
df_filtered = df.filter(df["Age"] > 30)
df_filtered.show()

# Sorting the DataFrame
df_sorted = df.orderBy(df["Age"])
df_sorted.show()

# Grouping and aggregating data
df_grouped = df.groupBy("Gender").agg({"Age": "avg"})
df_grouped.show()

#-------------------------------------------------------------------------

# Reading a CSV file
df_csv = spark.read.csv("data.csv", header=True, inferSchema=True)

# Writing a DataFrame to a CSV file
df_csv.write.csv("output.csv", header=True)

# Reading a Parquet file
df_parquet = spark.read.parquet("data.parquet")

# Writing a DataFrame to a Parquet file
df_parquet.write.parquet("output.parquet")

# Reading a JSON file
df_json = spark.read.json("data.json")

# Writing a DataFrame to a JSON file
df_json.write.json("output.json")

# Reading data from a database table (Java Database Connectivity) API)
df_jdbc = spark.read.format("jdbc").option("url", "jdbc:postgresql://localhost:5432/mydatabase") \
    .option("dbtable", "mytable").option("user", "myuser").option("password", "mypassword").load()

# Writing data to a database table
df_jdbc.write.format("jdbc").option("url", "jdbc:postgresql://localhost:5432/mydatabase") \
    .option("dbtable", "mytable").option("user", "myuser").option("password", "mypassword").mode("append").save()

#----------------------------------------------------------------------------------

from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# Define the schema
schema = StructType([
    StructField("Name", StringType(), nullable=False),
    StructField("Age", IntegerType(), nullable=True),
])

# Create a DataFrame with the defined schema
df_with_schema = spark.createDataFrame(data, schema)

# Renaming a column
df_renamed = df.withColumnRenamed("Name", "Full Name")
df_renamed.show()

# Dropping a column
df_dropped = df.drop("Age")
df_dropped.show()

# Checking for null values
df_null = df.filter(df["Age"].isNull())

# Filling null values with a default value
df_filled = df.fillna({"Age": 0})

# Dropping rows with null values
df_no_null = df.dropna()

#-------------------------------------------------------------------------------------
# Dropping rows with missing data
df_no_missing = df.dropna()

# Filling missing data with a default value
df_filled = df.fillna(0)

# Filling missing data with the mean value of a column
mean_age = df.select("Age").agg({"Age": "mean"}).first()[0]
df_filled = df.fillna

from pyspark.ml.feature import Imputer

# Create an Imputer object
imputer = Imputer(inputCols=["Age"], outputCols=["Age_imputed"])

# Fit the imputer model on the DataFrame
imputer_model = imputer.fit(df)

# Apply the imputation on the DataFrame
df_imputed = imputer_model.transform(df)


#---------------------------------------------------------------------------------------

# Grouping and aggregating data
df_grouped = df.groupBy("Gender").agg({"Age": "avg", "Salary": "sum"})
df_grouped.show()

# Calculating descriptive statistics
df_stats = df.describe(["Age", "Salary"])
df_stats.show()

# Creating a pivot table
df_pivot = df.groupBy("Gender").pivot("City").agg({"Salary": "sum"})
df_pivot.show()

#-------------------------------------------------------------------------------------
# Performing an inner join
df_joined = df1.join(df2, df1["ID"] == df2["ID"], "inner")
df_joined.show()

# Performing an outer join
df_joined = df1.join(df2, df1["ID"] == df2["ID"], "outer")
df_joined.show()

# Combining DataFrames using union
df_combined = df1.union(df2)
df_combined.show()


#-------------------------------------------------------------------------------------------

from pyspark.sql.functions import from_json
from pyspark.sql.types import StructType, StringType, IntegerType

# Define the schema for the streaming data
schema = StructType().add("name", StringType()).add("age", IntegerType())

# Create a Streaming DataFrame from a Kafka topic
streaming_df = spark.readStream.format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "topic-name") \
    .load() \
    .select(from_json(col("value").cast("string"), schema).alias("data"))
#------------------------------------------------------------------------------------------------

# Write the transformed streaming data to the console
query = streaming_df.writeStream.outputMode("append").format("console").start()
query.awaitTermination()
